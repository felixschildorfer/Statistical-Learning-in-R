---
title: "Model Selection"
output: html_document
---
Model Selection
===============

```{r}
library(ISLR)
library(leaps)
summary(Hitters)
```
Some values are missing, so we will remove them.

```{r}
Hitters = na.omit(Hitters)
with(Hitters,sum(is.na(Salary)))
```

Best Subset regression
----------------------

```{r}
regfit.full = regsubsets(Salary~., data = Hitters)
summary(regfit.full)
```
It gives best subsets up to size 8, but we want all of them.
```{r}

regfit.full = regsubsets(Salary~., data = Hitters,nvmax = 19)
reg.summary= summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp,xlab = "Number of Variables", ylab = "Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col="red")
```
There is a plot method for the 'regsubsets' object
````{r}
plot(regfit.full,scale="Cp")
coef(regfit.full,10)

````

Forward Stepwise Selection
--------------------------
Here we use the 'regsubsets' function but specify the 'method = forward'
````{r}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax = 19,method = "forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="Cp")

````

````{r}
dim(Hitters)
set.seed(1)
train=sample(seq(263),180,replace = F)
train
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax = 19, method = "forward")
````
......Stuff..Validation methods

````{r}
val.errors=rep(NA,19)
x.test = model.matrix(Salary~.,data = Hitters[-train,])
for (i  in 1:19) {
  coefi= coef(regfit.fwd,id=i)
  pred=x.test[,names(coefi)]%*%coefi
  val.errors[i]=mean(Hitters$Salary[-train]-pred^2)
}
plot(sqrt(val.errors),ylab = "Root MSE", ylim = c(300,400), pch = 19, type = "b")
points(sqrt(regfit.fwd$rss[-1]/180),col="red",pch=19,type = "b")
predict.regsubsets=function(object,newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  mat[,names(coefi)]%*%coefi
  }

````

Cross Validation
----------------

10 fold cross-validation
````{r}
set.seed(1)
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
cv.errors=matrix(NA,10,19)
for (k in 1:10) {
  best.fit = regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax = 19, method = "forward")
 for (i in 1:19) {
  pred=predict(best.fit,Hitters[folds==k,],id=i)
  cv.errors[k,i]=mean((Hitters$Salary[folds==k]-pred)^2)
 }
}
rmse.cv=sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,pch=19,type="b")

````
````{r}
library(glmnet)
x=model.matrix(Salary~.,data=Hitters)
y=Hitters$Salary

````
Let's fit a ridge model
````{r}
fit.ridge=glmnet(x,y,alpha = 0)
plot(fit.ridge,xvar = "lambda",label=T)
cv.ridge=cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
````
Lasso Model
````{r}
fit.lasso=glmnet(x,y)
plot(fit.lasso,xvar="lambda",label=T)
cv.lasso=cv.glmnet(x,y)
plot(cv.lasso)
coef(cv.lasso)
````
Train and Validation
`````{r}
lasso.tr=glmnet(x[train,],y[train])
lasso.tr

pred=predict(lasso.tr,x[-train,])
dim(pred)
rmse=sqrt(apply((y[-train]-pred)^2,2,mean))
plot(log(lasso.tr$lambda),rmse,type="b",xlab="logLambda")

lam.best=lasso.tr$lambda[order(rmse)[1]]
lam.best
coef(lasso.tr,s=lam.best)
````




